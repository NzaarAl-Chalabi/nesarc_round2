#this is a pipeline developed by Nzaar Al-Chalabi for the imputation and quality control of genetic data for polygenic risk scores.
#we will use tools such as Plink, bcftools and prsice2 to complete all the necessary steps.
#also, some of these steps are extremely computationally demanding, depending on the sample size. If you are working with hundreds and thousands of genetic samples, then I would say this is near impossible without knowing how to submit jobs to the cluster. 
      #therefore, before I begin the tutorial regarding the manipulation of genetic files, I will first show you how to submit a job to the cluster.
      
      
#Section 1: How to Submit a Job to the Cluster
#https://kcniconfluence.camh.ca/display/SCC/Running+Jobs+on+SCC+Clusters
#ok, so what is a job? whenever you log into the cluster, your first 2 lines of code look something like this

ssh dev01 
cd /external/mgmt3/genome/scratch/Deluca/nalchalabi/TutorialPGRS/

#you have accessed a development node and changed your directory to the necessary location to do your work.
#however, when you code on the development node, you aren't actually using the total computing power of the cluster. The cluster is a "supercomputer" because it's a bunch of computers connected to perform computationally demanding tasks. 
#the development nodes "ssh dev01 and ssh dev02" are just small parts of the cluster that are designed for you to put together a job submission to the cluster... 
#using a job submission allows you to completely shut down your computer or log off MOBA, and the tasks will be completed. It's much faster because you can use 100x the computing power to do multiple jobs concurrently.

#so, to upload a job, you have to use the following command

sbatch myscript.sh  

# let's break this step down. sbatch is the command that sends your script file to the cluster to read. all the coding you want the cluster to do is in the myscript.sh file. 
            #this is just a file you can create on the cluster or a text editor with all the commands you want to run. 
#how do I make a script file in the cluster? is something that you might be asking.
#no worries. To do that, follow the steps below:

nano myscript.sh 

#nano is a script editor tool that will create or edit a .sh file in your directory titled "myscript".  
#type all the commands you want into this script, including the parameters you are setting for your job submission

then press control + X, then press Y, then press enter 
#Control + X is how to save your script edits 
#Y is to confirm the edits 
#enter is to close the editor 

#ok, now you know how to make a script file; what do you put into it?
#you need to include all the necessary parameter functions and the code itself.
# I will attach the script we will use for the later stages below and annotate what everything means. do not worry so much about the code that's being executed but rather about the first lines
      #where we are telling the cluster how we want the job to be completed.
#also, please access https://kcniconfluence.camh.ca/display/SCC/Running+Jobs+on+SCC+Clusters for more information on job submission and the cluster in general.

---------- Everything between these lines you would copy and paste directly into your myscript.sh file ---------------

#!/bin/bash

#!/bin/bash -l
#SBATCH --job-name=attempt2NESARCHIGHSTRESS
#SBATCH --mail-type=END,FAIL
#SBATCH --mail-user=nzaar.alchalabi@mail.utoronto.ca
#SBATCH --ntasks=1
#SBATCH --mem=1gb
#SBATCH --time=500:00:00
#SBATCH --output=serial_test_%j.log
#SBATCH --cpus-per-task=24
#SBATCH --partition=verylong

# Print current working directory, hostname, and date
pwd
hostname
date

# Check loaded modules
module load bio/PLINK/1.9b_6.22-x86_64
module load bio/BCFtools/1.14-GCC-10.2.0

#everything below is the code that the job submission will execute so what is below is variable depending on the task you want to perform 

bcftools concat chr1.dose.vcf.gz chr2.dose.vcf.gz chr3.dose.vcf.gz chr4.dose.vcf.gz chr5.dose.vcf.gz chr6.dose.vcf.gz chr7.dose.vcf.gz chr8.dose.vcf.gz chr9.dose.vcf.gz chr10.dose.vcf.gz chr11.dose.vcf.gz chr12.dose.vcf.gz chr13.dose.vcf.gz chr14.dose.vcf.gz chr15.dose.vcf.gz chr16.dose.vcf.gz chr17.dose.vcf.gz chr18.dose.vcf.gz chr19.dose.vcf.gz chr20.dose.vcf.gz chr21.dose.vcf.gz chr22.dose.vcf.gz -Ou | 
bcftools view -Ou -i 'R2>0.3' |
bcftools norm -Ou -m -any |
bcftools norm -Ou -f  /external/mgmt3/imaging/scratch/Epidemiology/zagabani/HighStressNesarc/human_g1k_v37.fasta |
bcftools annotate -Oz -x ID -I +'%CHROM:%POS:%REF:%ALT' -o second.converted.R2_0.3.vcf.gz

plink --vcf second.converted.R2_0.3.vcf.gz \
--double-id \
--allow-extra-chr 0 \
--maf 0.01 \
--make-bed \
--out secondattempt_new_allchromosomes.MAF_0.01

---------------------------------------------------------------------------------------------

#ok so let us go through what the steps above are outlining. 
#!/bin/bash -l                                                      this indicated to the cluster that your code is using the bash language. if you wanted python for example you would convert this to the python indicator 
#SBATCH --job-name=attempt2NESARCHIGHSTRESS                         this is telling the cluster what you want to name your job, in our case we named it "attempt2nNESARCHIGHSTRESS"
#SBATCH --mail-type=END,FAIL                                        this is telling the cluster that you want an email notification if the job was to end or fail 
#SBATCH --mail-user=nzaar.alchalabi@mail.utoronto.ca                i think you can guess what this line is used for
#SBATCH --ntasks=1                                                  this line is indicating that you want to use 1 node and you want your tasks to be run on the same node 
#SBATCH --mem=1gb                                                   this line indicates the amount of RAM memory to use
#SBATCH --time=500:00:00                                            *this is extremely important* please pay attention. This line indicates to the cluster that my job needs to run for x amount of time. if you put lets say 1:00:00 for 1 day, and your task takes longer than 1 day 
                                                                                 , then when the time runs out, your job will be halted, and you will lose all that progress you had made. So in any job always put way more time needed than what you actually think you will need to ensure no premature 
                                                                                 stoppages occur. you can see here we put 500 days.
#SBATCH --output=serial_test_%j.log                                  this line indicates to the cluster that we want a you to generate a .txt file called serial_test.log that will give us the same outputs that our code would give us if we ran it on the development nodes. for example;
                                                                          "task 1 has been completed, starting task 2" and whatnot.
#SBATCH --cpus-per-task=24                                          this is a key line, it indicates to the cluster how much computing power you want to access. 24 cpus is a good amount but i recommend checking the website to find ways to get more out of the cluster.
#SBATCH --partition=verylong                                        this line is similar to the time line in that it is telling the cluster that this task will take a very long time.

# Print current working directory, hostname, and date               these three lines are house keeping lines that are necessary with all submissions
pwd
hostname
date

# Check loaded modules                                             this is also key, the cluster does not know what programs you need to do the coding. so you have to specify all the programs you need. (if you need R) then you can submit a job in R. 
                                                                   in the code above, I needed plink and bcftools so I had to load those modules into the cluster prior to the code starting.
module load bio/PLINK/1.9b_6.22-x86_64
module load bio/BCFtools/1.14-GCC-10.2.0


#once your script is ready and all the necessary files are ready in your directory you can simply type:

sbatch myscript.sh 

#this will submit your script and job.
#remember when you submit your job you need to be in the right working directory with all the necessary files, and the outputs will be in the same directory.

#to check on job status, you can do commands such as

squeue -l -u <username>
squeue -l -u nalchalabi


#and there you have it you have submitted a job to the cluster and it is running in the background. you should receieve a job submission ID in the cluster and be good to go.
#now you wait till your job is done or submit another job. 



#Section 2: How to manipulate files for imputation and how to submit an imputation job to the Michigan impute server
#https://imputationserver.readthedocs.io/en/latest/prepare-your-data/

#ok so the first step is to ensure you are in the correct working directory, with all your genetic data in that directory and you have loaded the necessary modules.

module load bio/PLINK/1.9b_6.22-x86_64
module load bio/BCFtools/1.14-GCC-10.2.0

#before we impute, let's first understand why we impute genetic data. 
#Imputation of genetic data is a common practice in genetics research and analysis. It involves predicting or inferring missing genotypes in a dataset based on reference panels or other available genetic information.
#Imputation serves several purposes and can provide various benefits: increased snp coverage, comparison and meta-analysis, impute rare variants, and quality control
#Before we proceed we need to make sure we have the genetic samples that we truly want in the analysis.
#on Excel create a .txt file of all the samples you want to impute, include their FID and IIDs in a two column .txt file. 
#then run this plink command to keep the files you want

plink --bfile input_file --keep sample_list.txt --make-bed --out output_file
#start by downloading the following tools with the code below 

#before we can impute we have to prepare our data


wget http://www.well.ox.ac.uk/~wrayner/tools/HRC-1000G-check-bim-v4.2.7.zip
wget ftp://ngs.sanger.ac.uk/production/hrc/HRC.r1-1/HRC.r1-1.GRCh37.wgs.mac5.sites.tab.gz

#output is these two following files

HRC-1000G-check-bim-v4.2.7.zip
HRC.r1-1.GRCh37.wgs.mac5.sites.tab.gz

unzip HRC-1000G-check-bim-v4.2.7.zip
gunzip HRC.r1-1.GRCh37.wgs.mac5.sites.tab.gz


#this will download a Perl script and a reference map file.
#the reference map file will be used to check your data is in the right build format (Ch37) and the Perl script will execute the checking and partitioning of your data into individual chromosomes.
#next we need to create a frequency file for our samples. assume our plink files are called NzaarSample.bed/ NzaarSample.bim / NzaarSample.fam

plink --freq --bfile NzaarSample --out NzaarSample_freq

perl HRC-1000G-check-bim.pl -b NzaarSample.bim -f NzaarSample_freq.frq -r HRC.r1-1.GRCh37.wgs.mac5.sites.tab -h
sh Run-plink.sh

#the output for this step is 22 individually updated chromosomes, you are going to have  to convert all of the 22 into vcfs and then bgzip them to compress them.
# I recommend using Excel to basically write the same command line over and over again but changing a single number
# I like to write out the parts of the code that stay the same and then the part that changes and use the =CONCAT function on Excel to combine everything into the right code.

plink --bfile  NzaarSample-updated-chr1 --recode vcf --out NzaarSampleOut1
plink --bfile  NzaarSample-updated-chr2 --recode vcf --out NzaarSampleOut2
plink --bfile  NzaarSample-updated-chr3 --recode vcf --out NzaarSampleOut3
plink --bfile  NzaarSample-updated-chr4 --recode vcf --out NzaarSampleOut4
plink --bfile  NzaarSample-updated-chr5 --recode vcf --out NzaarSampleOut5
plink --bfile  NzaarSample-updated-chr6 --recode vcf --out NzaarSampleOut6
plink --bfile  NzaarSample-updated-chr7 --recode vcf --out NzaarSampleOut7
plink --bfile  NzaarSample-updated-chr8 --recode vcf --out NzaarSampleOut8
plink --bfile  NzaarSample-updated-chr9 --recode vcf --out NzaarSampleOut9
plink --bfile  NzaarSample-updated-chr10 --recode vcf --out NzaarSampleOut10
plink --bfile  NzaarSample-updated-chr11 --recode vcf --out NzaarSampleOut11
plink --bfile  NzaarSample-updated-chr12 --recode vcf --out NzaarSampleOut12
plink --bfile  NzaarSample-updated-chr13 --recode vcf --out NzaarSampleOut13
plink --bfile  NzaarSample-updated-chr14 --recode vcf --out NzaarSampleOut14
plink --bfile  NzaarSample-updated-chr15 --recode vcf --out NzaarSampleOut15
plink --bfile  NzaarSample-updated-chr16 --recode vcf --out NzaarSampleOut16
plink --bfile  NzaarSample-updated-chr17 --recode vcf --out NzaarSampleOut17
plink --bfile  NzaarSample-updated-chr18 --recode vcf --out NzaarSampleOut18
plink --bfile  NzaarSample-updated-chr19 --recode vcf --out NzaarSampleOut19
plink --bfile  NzaarSample-updated-chr20 --recode vcf --out NzaarSampleOut20
plink --bfile  NzaarSample-updated-chr21 --recode vcf --out NzaarSampleOut21
plink --bfile  NzaarSample-updated-chr22 --recode vcf --out NzaarSampleOut22

#then you have to bgzip everything. BGzip is a special type of compression tool found in bcftools that the Michigan impute server requires you to use. 
bgzip NzaarSampleOut1.vcf
bgzip NzaarSampleOut2.vcf
bgzip NzaarSampleOut3.vcf
bgzip NzaarSampleOut4.vcf
bgzip NzaarSampleOut5.vcf
bgzip NzaarSampleOut6.vcf
bgzip NzaarSampleOut7.vcf
bgzip NzaarSampleOut8.vcf
bgzip NzaarSampleOut9.vcf
bgzip NzaarSampleOut10.vcf
bgzip NzaarSampleOut11.vcf
bgzip NzaarSampleOut12.vcf
bgzip NzaarSampleOut13.vcf
bgzip NzaarSampleOut14.vcf
bgzip NzaarSampleOut15.vcf
bgzip NzaarSampleOut16.vcf
bgzip NzaarSampleOut17.vcf
bgzip NzaarSampleOut18.vcf
bgzip NzaarSampleOut19.vcf
bgzip NzaarSampleOut20.vcf
bgzip NzaarSampleOut21.vcf
bgzip NzaarSampleOut22.vcf

#then take all these 22 files that have been zipped and move them to a folder on your desktop.
#make an account on the Michigan imputation server first and foremost.
#once you have logged in go to the RUN drop-down and select the first option "Genotype Imputation (Minimac4)
      #from here you need to specify the correct build, in our case its build 37. 
      #indicate the reference population you want to use to impute the SNPs, if your sample is all european you would impute based on european reference panels.
      #if you have a mixed population then you would pick mixed, etc. 
      #




#Section 3: Post-imputation download and quality control

#you will get an email when your imputation job is complete and the password to unzip your files. Go to the jobs tab, and select your job.
#go to the results tab in the job and you will find links to complete the download.
#this is a step you should definitely use a job submission for because downloading can take a very long time and can be interrupted so using a job is vital to save time.
#click on the bottom right of the text box and there should be a curl command to download all the files. 
#create a job script and submit the download to the cluster.
#you will download the files however they are all zipped so you need to unzip all the files and then i highly recommend removing the zipped files once they have all been opened to save space on the cluster and keep your directory organized.
#your job submission script should look something like this

-----------------------------------------------------------------
#!/bin/bash

#!/bin/bash -l
#SBATCH --job-name=download-unzip-remove
#SBATCH --mail-type=END,FAIL
#SBATCH --mail-user=nzaar.alchalabi@mail.utoronto.ca
#SBATCH --ntasks=1
#SBATCH --mem=1gb
#SBATCH --time=500:00:00
#SBATCH --output=serial_test_%j.log
#SBATCH --cpus-per-task=24
#SBATCH --partition=verylong

# Print the current working directory, hostname, and date
pwd
hostname
date

curl --- download link ----

unzip -P "yourpassword" chr_1.zip
rm chr_1.zip
unzip -P "yourpassword" chr_2.zip
rm chr_2.zip
unzip -P "yourpassword" chr_3.zip
rm chr_3.zip

----------------------------------------------------------
#this will open chromosome files 1-->3 and then remove the zipped files, extend this pattern for all 22 chromosomes or however many you are imputing 
#double check you have all the files before the next steps. This is to ensure you have the imputed files ready to go for future steps.
#if you want to repeat the analysis with different quality control steps later on this is the spot where all files undergo the same steps so you can perform all the steps above with no manipulation but the
      #next series of steps can be manipulated to fit your desired needs 



#now we have to combine all the files given our desired parameters and create a bed/bim/fam file for our combined genotyped data.
#this is the longest and most demanding step of the whole analysis so you must submit a job.
#even with a job submission this will take a long time so double-check to make sure the parameters you have set and the code you entered is correct.
      #for reference we submitted 6000 samples for our analysis and this step took 35 hours using the job submission parameters we have outlined above.

#here we are combining all our vcf files, we will filter based on R2 values.
#you can read more on what R2 values represent but a quick explanation is:

#R-squared (RÂ²) is a statistical measure that represents the proportion of the variance in one variable that can be explained by another variable in a regression model. It is commonly used as an evaluation metric for model fit and predictive accuracy. In the context of imputation and genome-wide association studies (GWAS), R-squared values have specific interpretations:
#Imputation: Imputation is the process of estimating missing or incomplete data values based on other observed data. In imputation models, R-squared measures the goodness of fit or accuracy of the imputation model. It indicates how well the imputed values match the actual values in the dataset.
#Higher R-squared values in imputation models suggest a better fit, indicating that a larger proportion of the variance in the variable being imputed can be explained by the other variables used in the imputation model. A high R-squared implies that the imputed values are more likely to be accurate and reliable.
#Genome-wide Association Studies (GWAS): GWAS are conducted to identify genetic variants associated with specific traits or diseases. In GWAS, R-squared values are often used to assess the strength of association between genetic variants and the trait of interest.
#In this context, R-squared represents the proportion of the variance in the trait that can be explained by the genetic variant being tested. A high R-squared value for a genetic variant indicates that it explains a larger proportion of the variation in the trait, suggesting a stronger association.
#It's important to note that R-squared values in GWAS are typically small, as complex traits are influenced by multiple genetic and environmental factors, and individual genetic variants typically have small effect sizes.

#we also need to normalize our data based on a reference fasta file human_g1k_v37.fasta, this file can be downloaded from https://www.internationalgenome.org/. Type 37 fasta in the search and you can download the necessary file there.
#these steps were taken from 
-----------------------------------------------------------
#!/bin/bash

#!/bin/bash -l
#SBATCH --job-name=attempt2NESARCHIGHSTRESS
#SBATCH --mail-type=END,FAIL
#SBATCH --mail-user=nzaar.alchalabi@mail.utoronto.ca
#SBATCH --ntasks=1
#SBATCH --mem=1gb
#SBATCH --time=500:00:00
#SBATCH --output=serial_test_%j.log
#SBATCH --cpus-per-task=24
#SBATCH --partition=verylong

# Print current working directory, hostname, and date
pwd
hostname
date

# Check loaded modules
module load bio/PLINK/1.9b_6.22-x86_64
module load bio/BCFtools/1.14-GCC-10.2.0

#concat combines all the chromosomes into a single file
#view filters by info score
#norm normalizes indels. Split multiallelic sites into biallelic records. SNPs and indels merged into a single record
#create final gzipped VCF file and annotate. Remove the original SNP ID and assign the new SNP ID as chrom:position:ref:alt

bcftools concat chr1.dose.vcf.gz chr2.dose.vcf.gz chr3.dose.vcf.gz chr4.dose.vcf.gz chr5.dose.vcf.gz chr6.dose.vcf.gz chr7.dose.vcf.gz chr8.dose.vcf.gz chr9.dose.vcf.gz chr10.dose.vcf.gz chr11.dose.vcf.gz chr12.dose.vcf.gz chr13.dose.vcf.gz chr14.dose.vcf.gz chr15.dose.vcf.gz chr16.dose.vcf.gz chr17.dose.vcf.gz chr18.dose.vcf.gz chr19.dose.vcf.gz chr20.dose.vcf.gz chr21.dose.vcf.gz chr22.dose.vcf.gz -Ou | 
bcftools view -Ou -i 'R2>0.3' |
bcftools norm -Ou -m -any |
bcftools norm -Ou -f  /data/kronos/NGS_Reference/fasta/human_g1k_v37.fasta |
bcftools annotate -Oz -x ID -I +'%CHROM:%POS:%REF:%ALT' -o new_allchromosomes.converted.R2_0.3.vcf.gz

#--double-id means that both family and within-family IDs to be set to the sample ID

plink --vcf new_allchromosomes.converted.R2_0.3.vcf.gz \
--double-id \
--allow-extra-chr 0 \
--maf 0.01 \
--make-bed \
--out new_allchromosomes.converted.R2_0.3.MAF_0.01

-------------------------------------------------------------------------------------
#you now have the necessary BED/BIM/FAM files for your genetic data that have been imputed.
#we have more quality control steps to perform.
#3 key steps outlined by the KCNI PRS tutorial include; 1. remove variants with the same positions, 2. update sex and phenotype status because these arent maintained after imputation, 3. update rsID. 

#1. first step is to remove duplicate positions, we will use the code below but change the file names.

plink --bfile sample_imputed --exclude list_variants_with_same_positions.txt --make-bed --out sample_imputed_clean1

#sample_imputed is the name of the binary files so in our case it would be new_allchromosomes.converted.R2_0.3.MAF_0.01
#you need to create a list of variant .txt file using a bash command such as the one below
#this awk command will search our bim file and if there is a string in the second column that is duplicated then it will print that row in a new .txt file 

awk '{ if ($2 in seen) print > "duplicates.txt"; else seen[$2] }' new_allchromosomes.converted.R2_0.3.MAF_0.01.bim

#now you can run this code to remove all duplicates

plink --bfile new_allchromosomes.converted.R2_0.3.MAF_0.01 --exclude duplicates.txt --make-bed --out sample_imputed_clean1



#2. the next step is to update the sex and phenotype IDs.

#i would suggest using excel, open the fam file and copy all the contents to an excel workbook, with the fam file that you had prior to all these steps change the sex and phenotype values and then copy this back into the fam file.
#if this doesnt work then you can skip this step until just prior to using PRSICE2.

#3. to update rsIDs 
#i will provide the file foe updating rsIDs in this repository. esentially you create a file with 2 columns. CHR:POS:allele1:allele2 in the first column and their corresponding rsID in column 2. 
# the reason is that the michigan impute server identifies snp via this format but PRSICE and other softwares use rsIDs so we need to convert these with a reference map file.


plink --bfile sample_imputed_clean1 --update-map shortnew2.txt --update-name --make-bed --out sample_imputed_clean2


#section 4: PRSice2 for PRS Generation

ok so now we have our imputed genetic samples that are ready to the main QC pipeline outlined in choi et al. (https://choishingwan.github.io/PRS-Tutorial/). 
This tutorial is very detailed and well done when it comes to completing the PRS pipeline.
however, I understand some things might not be as intuitive as the tutorial assumes them to be so I have provided more explanation and guidance below.

first we must quality control our summary statistics. 
download the summary statistics file from Dropbox or whereever you have been given them from. Usually, you will email the author of a major GWAS study and 
they will share their summary statistics table with you.

you will have a table of approximately 8 million SNPs that you must manipulate so that it is in the proper format. (See below)

CHR	BP	SNP	A1	A2	N	SE	P	OR	INFO	MAF
1	756604	rs3131962	A	G	388028	0.00301666	0.483171	0.997886915712657	0.890557941364774	0.369389592764921
1	768448	rs12562034	A	G	388028	0.00329472	0.834808	1.00068731609353	0.895893511351165	0.336845754096289
1	779322	rs4040617	G	A	388028	0.00303344	0.42897	0.997603556067569	0.897508290615237	0.377368010940814

This is approximately what we want our table to look like. however sometimes you are not provided INFO score so you can remove that column.
This just means you will not be filtering for that value in this QC pipeline. 

#some tips on manipulating a large text file to look like this.
#use chatgpt to help you come up with the necessary commands for moving columns, changing values in columns etc. 
#if you are not comfortable with that then a really brute force method could be to download the summary statistics text file, 
      copy 1 million ROWs at a time and paste them into excel. You can easily manipulate the amount of rows in excel with its user friendly UI 
      and then save the changes and move the data to a new text file. The problem with this is you will have to do that 8 times in a row to update 
      all 8 million rows of SNPS. 

once you have updated the table to look similar to the values above we can proceed with QC.

start by zipping your txt file to create a new compressed file.

gzip SummaryStatistics.txt 

this creates the file SummaryStatistics.txt.gz

now we start with QC.

gunzip -c SummaryStatistics.txt.gz |\
awk 'NR==1 || ($11 > 0.01) && ($10 > 0.8) {print}' |\
gzip  > Height.gz

###Decompresses and reads the Height.gwas.txt.gz file
#Prints the header line (NR==1)
#Prints any line with MAF above 0.01 ($11 because the eleventh column of the file contains the MAF information)
#Prints any line with INFO above 0.8 ($10 because the tenth column of the file contains the INFO information)
#Compresses and writes the results to Height.gz

important things to note, if you don't have a INFO file then your columns will be numbered slightly differently.
if you do not have an info file then you only quality control based on MAF scores for this step so your code will loook like this. 


gunzip -c SummaryStatistics.txt.gz |\
awk 'NR==1 || ($10 > 0.01) {print}' |\
gzip  > Height.gz


#this is the modified code assuming no INFO scores and you want to filter MAF to those greater than 0.01.
#also I have used the same naming convention as the tutorial in CHOI et al, if you want to change names that can be easily done by manipulating the output file commands. 


#now remove duplicate snps with this code 

gunzip -c Height.gz |\
awk '{seen[$3]++; if(seen[$3]==1){ print}}' |\
gzip - > Height.nodup.gz

#The above command does the following:
#Decompresses and reads the Height.gz file
#Count number of time SNP ID was observed, assuming the third column contian the SNP ID (seen[$3]++). If this is the first time seeing this SNP ID, print it.
#Compresses and writes the results to Height.nodup.gz

#again check to make sure your SNP IDs are in the third column before doing this.

finally remove ambiguous snps with this command.

gunzip -c Height.nodup.gz |\
awk '!( ($4=="A" && $5=="T") || \
        ($4=="T" && $5=="A") || \
        ($4=="G" && $5=="C") || \
        ($4=="C" && $5=="G")) {print}' |\
    gzip > Height.QC.gz

#this creates your final quality-controlled sample statistics file that we will use for PRSice. 


#now we will create our QC target data 

standard QC steps 

plink \
    --bfile EUR \
    --maf 0.01 \
    --hwe 1e-6 \
    --geno 0.01 \
    --mind 0.01 \
    --write-snplist \
    --make-just-fam \
    --out EUR.QC


go ahead and run this code 

plink \
    --bfile EUR \
    --keep EUR.QC.fam \
    --extract EUR.QC.snplist \
    --indep-pairwise 200 50 0.25 \
    --out EUR.QC


plink \
    --bfile EUR \
    --extract EUR.QC.prune.in \
    --keep EUR.QC.fam \
    --het \
    --out EUR.QC


#open R module on your cluster and run this 

dat <- read.table("EUR.QC.het", header=T) # Read in the EUR.het file, specify it has header
m <- mean(dat$F) # Calculate the mean  
s <- sd(dat$F) # Calculate the SD
valid <- subset(dat, F <= m+3*s & F >= m-3*s) # Get any samples with F coefficient within 3 SD of the population mean
write.table(valid[,c(1,2)], "EUR.valid.sample", quote=F, row.names=F) # print FID and IID for valid samples
q() # exit R

all these steps are outlined in CHOI et al in detail in what they do.
#i believe PRSice performes a check of mismatching SNPs so that step wouldnt be necessary for us to perform.

to generate the final step of QC data use 

plink \
    --bfile EUR \
    --make-bed \
    --out EUR.QC \
    --extract EUR.QC.snplist \

#this will create a basic cleaned file of our data. 

#now we need to create all the necessary files for PRsice analysis.
#to start, download the PRsice linux file from Choi et al and import that into your directory.
#unzip the file and you should be ready to go in regards to using the program

#below is the list of files we need to perform prsice. we already have the first 4 based on quality controlling our base and target data.
#to create EUR.cov and EUR.height just use excel. All you need to do is include the FIID and IID followed by the sex (for the sex file) or the 
      variables of interest for the .cov file. (phenotypes etc).

Height.QC.gz	The post QC base data file. While PRSice-2 can automatically apply most filtering on the base file, it cannot remove duplicated SNPs
EUR.QC.bed	This file contains the genotype data that passed the QC steps
EUR.QC.bim	This file contains the list of SNPs that passed the QC steps
EUR.QC.fam	This file contains the samples that passed the QC steps
EUR.height	This file contains the phenotype data of the samples
EUR.cov	This file contains the covariates of the samples
EUR.eigenvec	This file contains the principal components (PCs) of the samples



#to create the eigenvec file use the code below 

# First, we need to perform prunning
plink \
    --bfile EUR.QC \
    --indep-pairwise 200 50 0.25 \
    --out EUR
# Then we calculate the first 6 PCs
plink \
    --bfile EUR.QC \
    --extract EUR.prune.in \
    --pca 6 \
    --out EUR

#if you want more PC values change the 6 to 10 or w.e you want. 

we need to now combine the covariables in analysis so run this R commmand.

covariate <- read.table("EUR.cov", header=T)
pcs <- read.table("EUR.eigenvec", header=F)
colnames(pcs) <- c("FID","IID", paste0("PC",1:6))
cov <- merge(covariate, pcs, by=c("FID", "IID"))
write.table(cov,"EUR.covariate", quote=F, row.names=F)
q()


remember if you do more than 6 PCs you have to change the 3rd line of code to 1:10 or whatever the number of PCs you want to include.

now you should be good to run PRSICE!!

Rscript PRSice.R \
    --prsice PRSice_linux \
    --base Height.QC.gz \
    --target EUR.QC \
    --binary-target F \
    --pheno EUR.height \
    --cov EUR.covariate \
    --stat OR \
    --or \
    --out EUR

run this and you will get your results. 



















     
